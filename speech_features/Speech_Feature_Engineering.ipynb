{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9545e3-0dd9-43f9-a573-03b71b8ffdb8",
   "metadata": {},
   "source": [
    "# Speech Feature Engineering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f1e37-7afe-48d5-a966-81f4348f250e",
   "metadata": {},
   "source": [
    "## Convert Audio File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4accda4f-d722-4d1f-9a7c-72563ba5e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5766ad9-32b3-4e10-81d1-eb8538750bd2",
   "metadata": {},
   "source": [
    "For pydub was getting \"No such file or directory: 'ffprobe'\" error with pydub so instead used `conda install -c conda-forge ffmpeg` (rather than pip) to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38c0bd5-2971-4ce5-befc-c660fa86c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for audio files\n",
    "m4a_path = './data/m4a_files/'\n",
    "wav_path = './data/wav_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc41d531-63a2-4826-8229-3e31c89320c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_m4a_to_wav(file_name, input_dir, output_dir):\n",
    "    '''\n",
    "    Convert an .m4a audio file to a .wav audio file using PyDub.\n",
    "\n",
    "    Inputs:\n",
    "        file_name (str): name of file (without extension)\n",
    "        input_dir (str): directory path for input .m4a file\n",
    "        output_dir (str): directory path for output .wav file\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Load the m4a file\n",
    "    audio = AudioSegment.from_file(input_dir + file_name + '.m4a', format = 'm4a')\n",
    "#     # Export the audio to wav format\n",
    "#     audio.export(output_dir + file_name + '.wav', format = 'wav')\n",
    "    \n",
    "    # Set the desired sample rate and bit depth\n",
    "    desired_sample_rate = 44100\n",
    "    desired_sample_width = 2  # 16-bit depth\n",
    "\n",
    "    # Resample the audio to the desired sample rate\n",
    "    resampled_audio = audio.set_frame_rate(desired_sample_rate)\n",
    "\n",
    "    # Set the bit depth to the desired value\n",
    "    converted_audio = resampled_audio.set_sample_width(desired_sample_width)\n",
    "\n",
    "    # Export the converted audio to a new WAV file\n",
    "    converted_audio.export(output_dir + file_name + '.wav', format=\"wav\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6b6fb5-0e40-424a-bec2-402c3e2ab411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty directories for wav files (if it doesn't exist)\n",
    "if not os.path.exists(wav_path):\n",
    "    os.makedirs(wav_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea2c612-8cd7-429e-9efb-9729edd46142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "file_list = os.listdir(m4a_path) # List all files in original directory\n",
    "# Updated list of files names\n",
    "# remove extension and skip files that start with '.' (e.g. ipynb checkpoints)\n",
    "file_list = [x.replace('.m4a', '')for x in file_list if x[0] != '.'] # Remove extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239f8953-bcfb-4625-bde8-c2de786cf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_temp = file_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc34474e-9173-4ee4-9ab8-b55574c3306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all audio files to wav format\n",
    "for file in file_list_temp:\n",
    "    convert_m4a_to_wav(file, m4a_path, wav_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59548040-5547-4fe8-937a-5cedd8e93f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f21ffa4-df84-4a69-86c4-272bb938955d",
   "metadata": {},
   "source": [
    "## Testing on a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaedce31-4f31-4805-bc40-2d82d26e4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"348th_11.4.21\" # Audio File title\n",
    "wav_file = wav_path + file_name + '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffb00a-42e4-4282-82df-d84140f5a155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4a30f7-ac60-409d-b4f4-a913488499ae",
   "metadata": {},
   "source": [
    "## My-Voice-Analysis Library\n",
    "\n",
    "https://github.com/Shahabks/my-voice-analysis\n",
    "\n",
    "From [paper](https://www.jmir.org/2021/4/e24191/) analyzing stress of health care professionals:\n",
    "\n",
    "For the temporal features, the My-Voice Analysis [28] package was used. This package was built off of the speech analysis research tool praat [29]. Temporal features were actualized as the speech rate, syllable count, rate of articulation, speaking duration, total duration, and ratio of speaking to nonspeaking. This package was also used to extract prosodic features, namely the F0 values: mean, standard deviation, minimum, maximum, and upper and lower quartiles. The F0 value is the representation of what is known as the pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426405b-1b3f-4520-9c9f-60f62a030622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install my-voice-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbe460-9793-4a75-91aa-22c1cebf8761",
   "metadata": {},
   "source": [
    "At first I was getting a \"Try again the sound of the audio was not clear\" response whenever I ran any of the functions from this package. I ended up copying the code from that package's repo and modified all the `sourcerun` file paths in the functions and now it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3300ab90-9d35-4d4a-a68a-d026698edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset = mysp.mysptotal(file_name, wav_path[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7378b5bc-5a6d-45f2-af1f-81cb91fd08e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_ of_syllables</th>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_pauses</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_of_speech</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>articulation_rate</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaking_duration</th>\n",
       "      <td>150.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_duration</th>\n",
       "      <td>309.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_mean</th>\n",
       "      <td>224.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_std</th>\n",
       "      <td>62.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_median</th>\n",
       "      <td>222.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_min</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_max</th>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_quantile25</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_quan75</th>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "number_ of_syllables     736\n",
       "number_of_pauses         203\n",
       "rate_of_speech             2\n",
       "articulation_rate          5\n",
       "speaking_duration      150.8\n",
       "original_duration      309.3\n",
       "balance                  0.5\n",
       "f0_mean               224.76\n",
       "f0_std                 62.28\n",
       "f0_median              222.7\n",
       "f0_min                    70\n",
       "f0_max                   423\n",
       "f0_quantile25            187\n",
       "f0_quan75                264"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dataset.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1361dd15-a487-4a73-9e2a-b0a2589fc5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a female, mood of speech: Reading, p-value/sample size= :0.00', 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gender recognition and mood of speech:\n",
    "gender_mood = mysp.myspgend(file_name, wav_path[:-1])\n",
    "gender_mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efa5cfe-e69d-4113-994c-33ac7f22870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mood(gender_mood_string):\n",
    "    '''\n",
    "    Want to extract the mood of speech from the gender and mood string from my-voice-analysis package\n",
    "    \n",
    "    For example, from the string:\n",
    "    ('a female, mood of speech: Reading, p-value/sample size= :0.00', 5)\n",
    "    I'd want to return \"Reading\"\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Find the index of the first colon and the next comma after it\n",
    "    colon_index = gender_mood_string.find(':')\n",
    "    comma_index = gender_mood_string.find(',', colon_index)\n",
    "\n",
    "    # Extract the text between the colon and comma using slicing\n",
    "    mood = gender_mood_string[colon_index+2:comma_index]\n",
    "    \n",
    "    return mood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818b6672-a060-422d-9d35-8702c0e1d93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mood = extract_mood(gender_mood[0])\n",
    "mood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b0430-d6cf-4752-aaa5-ffe3d5bbda23",
   "metadata": {},
   "source": [
    "## Python Speech Features Library\n",
    "\n",
    "https://github.com/jameslyons/python_speech_features\n",
    "\n",
    "[Mel Frequency Cepstral Coefficients (MFCC)](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)\n",
    "\n",
    "\n",
    "From [paper](https://www.jmir.org/2021/4/e24191/) analyzing stress of health care professionals:\n",
    "\n",
    "Formant features were calculated using the Python Speech Features library [30]. To characterize this aspect of speech, the original sound recording was refit according to a series of transformations commonly used for speech recognition that yield a better representation of the sound called the mel-frequency cepstrum (MFC). From this new representation of the sound form, the first 14 coefficients of the MFC were extracted. The MFC values were extracted given that they describe the spectral shape of the audio file, generally with diminishing returns in terms of how informative they are, which is why we only considered the first 14 coefficients. If we were to select a greater number of MFC values, it would result in a potentially needlessly more complex machine learning model using less informative features.\n",
    "\n",
    "From each of these waves, the mean, variance, skewness, and kurtosis were calculated for the energy (static coefficient), velocity (first differential), and acceleration (second differential).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da4c736-035a-49d9-a74d-e2a05d4dc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ec7f52-5546-4c22-bd43-a4b8ae31a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c480771-da4f-4278-9645-12eeff60abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rate,sig) = wav.read(wav_file)\n",
    "mfcc_feat = mfcc(sig, rate, nfft = 2000)\n",
    "fbank_feat = logfbank(sig, rate, nfft = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "192eb6cd-e984-40ec-b14f-5dfa056e3fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.29968018,  13.72290818, -29.19809589, ..., -13.55647358,\n",
       "         -4.20161053,  11.28364227],\n",
       "       [ 10.47709217,  14.78715985, -29.81894629, ..., -18.21122394,\n",
       "         -9.48733424,   4.28475425],\n",
       "       [ 11.78522646,  25.73448529,  -7.67875987, ...,  -5.00370421,\n",
       "         -1.36992302,   2.23719493],\n",
       "       ...,\n",
       "       [ 15.39286591,  24.08755919,  -0.10427238, ...,  -3.71280953,\n",
       "        -27.26275844,  -9.63292934],\n",
       "       [ 14.47122335,  19.26920955,   4.0923109 , ...,  -5.08731833,\n",
       "        -22.63729858,  -7.90971875],\n",
       "       [  3.04433085, -12.69258491,  -7.31047902, ..., -12.002807  ,\n",
       "         -9.2831477 ,  -4.79648642]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfb76f3e-5fa8-4e09-a9ad-5b187d0f002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30928, 13)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fbbc53c-ceeb-4be8-aa70-d3e2f2656ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract mean, stdev, skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd17a2-d158-4f5b-8542-71a13485a8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb55498-795a-43e1-930e-1263e07acf44",
   "metadata": {},
   "source": [
    "## Librosa Library\n",
    "\n",
    "https://pypi.org/project/librosa/\n",
    "\n",
    "From [paper](https://www.jmir.org/2021/4/e24191/) analyzing stress of health care professionals:\n",
    "\n",
    "The Librosa package [31] was used to calculate the mean, maximum, minimum, and standard deviation of the root mean square value, centroid, bandwidth, flatness, zero-crossing rate, loudness, and flux of the spectrogram, or the visualization of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7745cc-feed-457e-8d69-1a144dc151db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85983d6b-5cd2-4ff0-b535-88ad7d2a0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee76f2f-0ee3-48cc-b365-661f8a3e7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n",
      "Loudness: [[-48.007576 -39.336563 -32.142986 ... -22.90003  -24.209024 -26.71492 ]]\n",
      "Spectral centroid: [[1854.40866307  945.04239462  713.05587732 ... 1150.47042849\n",
      "  1063.63073103  891.03299265]]\n"
     ]
    }
   ],
   "source": [
    "# Load the WAV file using librosa\n",
    "y, sr = librosa.load(wav_file)\n",
    "\n",
    "# Extract pitch, loudness, and spectral centroid\n",
    "pitch = librosa.pitch.piptrack(y=y, sr=sr)\n",
    "loudness = librosa.amplitude_to_db(librosa.feature.rms(y=y))\n",
    "spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "# Print the features\n",
    "print(\"Pitch:\", pitch)\n",
    "print(\"Loudness:\", loudness)\n",
    "print(\"Spectral centroid:\", spec_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e328bb5-40b8-4f7b-8d87-f376f16e4b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9552ac-b70a-4fd8-a681-4f84045cf891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
